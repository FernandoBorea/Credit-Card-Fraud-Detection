---
title: "Credit Card Fraud Detection"
author: "Fernando JosÃ© Velasco Borea"
date: "May 12th 2019"
output: 
  pdf_document:
      toc: true
      number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\pagebreak

# Introduction and Overview

An article conducted by Loss Prevention Magazine in 2018 showed that by 2020 the total monetary losses due to credit card fraud in the U.S. alone could excede the $10,000,000,000 mark (you can find the article [here](https://losspreventionmedia.com/credit-card-fraud-statistics-and-insights/)). With a constant growth on cardholders across the years, the concern about this type of fraud has also increased. On 2017 we saw an increment of 1.3 million credit card fraud victims, implying an increase of 8.4% when compared to the 2016 period (as reported by Javelin Strategy & Research). Taking this into account, I decided to conduct a supervised machine learning project with the goal of predicting potential fraudulent credit card transactions.

For this project, we will be using the data set provided by Machine Learning Group - ULB through Kaggle (you can find it through this [link](https://www.kaggle.com/mlg-ulb/creditcardfraud)). The data set contains information about the time (relative to the frequency of the transactions when compared to the first one in the data set), amount, type of transaction (either fraudulent or non-fraudulent, represented by a 1 or a 0 respectively) and 28 numerical features resulting from a PCA Dimensionality Reduction to protect the users identity and sensitive information.

The project will be divided into 4 major sections, as follows:

1. Data Adquisition
2. Data Exploration and Wrangling
3. Modeling
4. Testing

Once we complete the sections mentioned above, we will create a _Conclusions_ section with the insights we gathered throughout the project.

## Side Notes

Although the data set used for this project is downloaded within the code, to improve the run time, it is recommended to clone the [GitHub repository](https://github.com/FernandoBorea/Credit-Card-Fraud-Detection) as it contains the `csv` file with the data set we used.

To enhance code readability when viewing the Rmd version of this report and/or when viewing the Credit Card Fraud Detection Script file to see only the coding part of the project, you can _fold_ the all the sections from RStudio to then just _unfold_ the section you are currently viewing, therefore, easing the interpretation of the code.

You can quickly do this from RStudio going to _Edit > Folding > Collapse All_ or simply with the shortcut _ALT + O_ on windows. If you want to exapnd all the sections again, you can use the shortcut _ALT + SHIFT + O_ on windows or from _Edit > Folding > Expand All_.

The code contained in this report can be found on the Credit Card Fraud Detection Script file. It follows the same structure and order as the report, therefore, making it easier to reproduce the results while maintaining code readability.

To render the Rmd version of this report you will need to have a LaTeX installation. If you don't have it, you can find more details on how to install it [here](https://bookdown.org/yihui/rmarkdown/installation.html#installation).

\pagebreak

# Data Adquisition

This section is going be mainly intended to download or read the data set (depending if you have the repository cloned into your local machine) that we will be using throughout the project. 

First, we will start by loading the required libraries for our project, and then proceed to read our data either from our working directory if we cloned the repository, or from Git LFS if we have not. Note that because of formatting purposes, we will not show the output messages from the code below on the report. 

Executing this code section might take some minutes depending on your internet connection.

```{r data adquisition libraries, message=FALSE, warning=FALSE, error=FALSE}

if(!require(tidyverse)) install.packages("tidyverse", 
                                         repos = "http://cran.us.r-project.org")

if(!require(RCurl)) install.packages("RCurl", 
                                     repos = "http://cran.us.r-project.org")

if(!require(knitr)) install.packages("knitr", 
                                     repos = "http://cran.us.r-project.org")

if(!require(caret)) install.packages("caret", 
                                     repos = "http://cran.us.r-project.org")

if(!require(randomForest)) install.packages("randomForest", 
                                     repos = "http://cran.us.r-project.org")

if(file.exists("creditcard.csv"))
{
  
  cc_dataset <- read_csv("creditcard.csv")
  
} else {
  
  URL_p1 <- "https://media.githubusercontent.com"
  URL_p2 <- "/media/FernandoBorea/Credit-Card-Fraud-Detection/master/creditcard.csv"
  datURL <- getURL(paste(URL_p1, URL_p2, sep = ""))
  
#We divided the entire URL in 2 string vectors and 
#then used paste to maintain the report formatting
  
  cc_dataset <- read_csv(datURL)
  
}

```

\pagebreak

## Preliminary Data Exploration

Once the Data Adquisition process is finished, we will start performing some preliminary data exploration to make sure the data was downloaded and/or read correctly and to familiarize ourselves with the data set. 

When calling the function `str()` to look for the data structure, it will result in quite a large and somewhat messy output within our report. We already know from the [Kaggle Site](https://www.kaggle.com/mlg-ulb/creditcardfraud) where we got our data from, that we have several columns in our data set, therefore we are not going to include the output of the code below.

```{r preliminary exploration 1, results='hide'}

str(cc_dataset)

```

As we did not show the output of the code above, we will use another approach to still show some information about the data set within this section, more specifically, we will just check the amount of rows and columns as well as the class of each column:

```{r preliminary exploration 2}

data.frame(Columns = ncol(cc_dataset), Rows = nrow(cc_dataset)) %>% 
  knitr::kable()

col_classes <- data.frame(Column = colnames(cc_dataset)[1:16],
                 Class = unname(apply(cc_dataset, 2, class))[1:16],
                 Column = c(colnames(cc_dataset)[17:ncol(cc_dataset)],""),
                 Class = c(unname(apply(cc_dataset, 2, class))[17:ncol(cc_dataset)],""))

colnames(col_classes) <- c("Column", "Class", "Column","Class")

col_classes %>% knitr::kable()

```

\pagebreak

# Data Exploration and Wrangling

Now, as we finished the Data Adquisition phase, we will dive into our data set to gather useful insights and perform some data wrangling if necessary for the Modeling phase. As we saw from our Preliminary Data Exploration section, we are dealing with a slightly large amount of variables.

If we evaluate the chart containing the classes for each column on the previous section, we can notice that all of them contain numeric values, but the last one (the Class column) contains a binary value, either 1 or 0 for fraudulent or non-fraudulent transactions respectively. For this reason, we need to convert it from numeric to factor. We can do this using the following code:

```{r data wrangling: class as factor}

cc_dataset <- cc_dataset %>% mutate(Class = as.factor(cc_dataset$Class))

class(cc_dataset$Class)

```

Next, we are going to explore the data with some plots. We will start by cheking out the distribution of the `Class` variable. We can do that with the follwing code:

```{r fraudulent transactions distribution, out.width = '85%', fig.align='center'}

cc_dataset %>%
  count(Class) %>%
  ggplot(aes(x = Class, y = n/100, fill = Class)) + 
  geom_col(col = "Black") + 
  scale_y_log10() + 
  labs(title = "Fraudulent Transactions Distribution", 
       x = "Transaction Type (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "Count in Hundreds")

```

We can inmediately see a huge difference on the data distribution. Because of this, we will now check how many non-fraudulent transactions and how many fraudulent transactions we have in our data.

```{r class count}

class_dist <- cc_dataset %>%
               group_by(Class) %>%
               summarize(Count = n())
class_dist %>% knitr::kable()

```

The previous result tells us that we are facing a data set with a massively unbalanced distribution on the `Class` column. Because of this, we are going to take a new approach in our data exploration. In order to avoid creating 30 plots to check the distribution of each variable relative to the Class column, we are going to group each variable by the type of the observation to then compute the average on each group, then, we will visualize the distribution of the ones that have the biggest difference on the averages. 

```{r avg differences}

#Because of formatting purposes, we will not print out this object

vars_grouped_avgs <- cc_dataset %>%
                     group_by(Class) %>%
                     summarize_all(list(mean))

#We will drop the class column as it is a factor and we cannot perform operations with it

vars_diff <- vars_grouped_avgs[,-1]

#Then, we will calculate the difference bewtween values and sort them

diff <- abs(vars_diff[1,] - vars_diff[2,])
diff <- sort(diff, decreasing = TRUE)
diff

```

\pagebreak

Now, as we can see, the biggest differences were on the `Time` and `Amount` variables, but when we look at those entries on the `vars_grouped_avgs` data frame, we notice that we might have a very similar distribution as the difference between the averages on those cases is not that large when we take into account the magnitude of the values. 

```{r time and amount lookup}

vars_grouped_avgs[c("Time", "Amount")]

```

We can check if our hyphothesis is correct by plotting those variables and compare the actual distribution when grouped by the transaction type.

```{r hypthothesis check, out.width='75%', fig.align='center'}

#We will transform Amount variable to log10 values and tidy up the data 

time_amount_tidy <- cc_dataset[c("Time", "Amount", "Class")] %>%
  mutate(Amount = if_else(Amount != 0,log10(Amount), Amount)) %>%
  gather(-Class, key = "Variable", value = "Values")

#To then plot it

time_amount_tidy %>% 
  ggplot(aes(x = Class, y = Values, fill = Class)) +
  facet_wrap(~Variable, scales = "free") +
  geom_boxplot() + 
  labs(title = "Time and Amount Variables Distribution", 
       x = "Transaction Type (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "value")

```

As we can see, our hyphothesis was correct as we see a very similar distribution on both variables when we plot them grouped by the transaction type. This fact makes them have less relevance as predictors, so we will drop them from our vector.

```{r time and amount drop}

diff <- diff[-which(names(diff) %in% c("Time", "Amount"))]

```

Once we have updated the `diff` vector, we will make a plot comparing the value distribution of the new top 3 variables we got from our calculation, once again, dividing the data into fraudulent and non-fraudulent transactions:

```{r plot top avg vars, fig.align='center', out.width='85%'}

#We will tidy up the data first

tidy_data <- cc_dataset[c("V3", "V14", "V17", "Class")] %>%
  gather(-Class, key = "Variable", value = "Values")

#And then plot it

tidy_data %>% ggplot(aes(x = Class, y = Values, fill = Class)) + 
  facet_wrap(~Variable, scales = 'free') +
  geom_boxplot() + 
  labs(title = "V3, V14 and V17 Variables Distribution", 
       x = "Transaction Type (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "value")

```

As we can tell from the previous plots we generated, the Interquartile Ranges of the data distribution when we group by transaction type are well separated from each other, meaning that we can in fact have some predictive power from those variables, validating our initial approach of calculating the variable averages difference when grouped by transaction type.

\pagebreak

Now, another study we can perform with our data to corroborate that our approach is correct, is to make the same plots but this time with the last 3 variables from our `diff` vector and see if we in fact get a very similar distribution, meaning that those variables have less predictive power. We can perform this with the following code:

```{r plot tail avg vars, fig.align='center', out.width='75%'}

#We will again tidy up the data first

tidy_data <- cc_dataset[c("V25", "V23", "V22", "Class")] %>%
  gather(-Class, key = "Variable", value = "Values")

#And then plot it

tidy_data %>% ggplot(aes(x = Class, y = Values, fill = Class)) + 
  facet_wrap(~Variable, scales = 'free') +
  geom_boxplot() + 
  labs(title = "V25, V23 and V22 Variables Distribution", 
       x = "Transaction Type (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "value")

```

As we can see, our approach seems to hold up, as the data on the last 3 variables on our vector have a very similar Interquartile Range, therefore, providing low predictive power. Lastly on this section, we will perform some aditional data wrangling prior starting our modeling phase, more specifically, we will create a training set as well as a test set. We can do this with the following code:

```{r train and test set creation}

y <- cc_dataset$Class

#We will use 70% of the data for training and 30% for testing

train_index <-  createDataPartition(y, times = 1, p = 0.7, list = FALSE)

train_set <- cc_dataset %>% slice(train_index)
test_set <- cc_dataset %>% slice(-train_index)

```

# Modeling

## General Approach

As we noticed on the Data Exploration phase, we are dealing with a very unbalanced data distribution, so evaluating our model just based on accuracy is not viable because if we predict all transactions as non-fraudulent (which would cost billions in losses to the banking industry), we would get an accuracy of $\sim 99.83\%$. Taking this into account, we can also evaluate our model using the Sensitivity value (or True Positive Rate), as our final goal is to predict as much true fraudulent transactions as we can.

We will focus on achieving a $Sensitivity >= 0.80$ while maintaining also a $Specificity >= 0.95$ so we also avoid alerting about too many false positives. Despite the low viability of predicting all transactions as non-fraudulent, we can use this very simplistic approach as our baseline to evaluate the ongoing models. 

To calculate all the metrics for this approach, we will create a Confusion Matrix where we predict all transactions as non-fraudulent. This gives us a potential walkaround to get more informatives results, as the Confusion Matrix also calculates a balanced accuracy that takes into account the data prevalence, so we can also use that value on our metrics. Taking this into account, we will also be focucing on ahcieving a $Balanced Accuracy >= 0.90$. We will store our results into a new chart.

```{r baseline approach}

all_nonfraud <- rep(0, nrow(test_set)) %>% factor(levels = c("0", "1"))

cm_all_nonfraud <- confusionMatrix(data = all_nonfraud, 
                                   reference = test_set$Class,
                                   positive = "1")

base_accuracy <- cm_all_nonfraud$overal["Accuracy"]
base_b_accuracy <- cm_all_nonfraud$byClass["Balanced Accuracy"]
base_sensitivity <- cm_all_nonfraud$byClass["Sensitivity"]
base_specificity <- cm_all_nonfraud$byClass["Specificity"]

metrics <- c("Model", "Accuracy", "Balanced Accuracy", "Sensitivity", "Specificity")

results <- data.frame(Model = "All Non-Fraudulent", 
                      Accuracy = unname(base_accuracy), 
                      "Balanced Accuracy" = unname(base_b_accuracy),
                      Sensitivity = unname(base_sensitivity), 
                      Specificity = unname(base_specificity),
                      stringsAsFactors = FALSE)
colnames(results) <- metrics
results %>% knitr::kable()

```

As we can see, including the balanced accuracy in our metrics gives us a way more informative result when compared to just the accuracy alone. It takes into account the data prevalence, therefore, sorting out our highly unbalanced data. As expected, we got a quite bad value on that metric, still, it will serve as a baseline measure.

\pagebreak

We will build initially 3 models based on all the variables of our data, then we will choose the best performing model among those and tune-up the variables we will be using as predictors as we clearly saw that some of them have little to no predictive power. The models we will use are based on a relatively simplistic mathematical approach, and because of our data set size, this will significally improve our run time. The models we will be training on are the following:

1. Generalized Linear Model
2. Linear Discriminant Analysis Model
3. Quadratic Discriminant Analysis Model

## Generalized Linear Model - All Variables

As explained on the General Approach section, we will now train a $GLM$ model using all the available variables to get a sense of the model performance we would get if we decide to use a Generalized Linear Model as a final model, then, we will store our results. 

Note that we will get this warning message: `fitted probabilities numerically 0 or 1 occurred`, this is due to the amount of predictors we are using, and as we are not going to consider in this case the Coefficients we get from our fitted model, we can disregard the warning. Note that running this code might take several minutes depending on your computer characteristics.

```{r glm model allvar}

fit_glm_allvar <- glm(Class ~ ., data = train_set, family = "binomial")

predict_glm_allvar <- predict(fit_glm_allvar, test_set, type = "response")

yhat_glm_allvar <- if_else(predict_glm_allvar > 0.5, 1, 0) %>% factor()

cm_glm_allvar <- confusionMatrix(data = yhat_glm_allvar, reference = test_set$Class, 
                                 positive = "1")

glm_allvar_acc <- cm_glm_allvar$overall["Accuracy"]
glm_allvar_b_acc <- cm_glm_allvar$byClass["Balanced Accuracy"]
glm_allvar_st <- cm_glm_allvar$byClass["Sensitivity"]
glm_allvar_sp <- cm_glm_allvar$byClass["Specificity"]

glm_allvar_results <-  data.frame(Model = "GLM - All Variables",
                                  Accuracy = unname(glm_allvar_acc),
                                  "Balanced Accuracy" = unname(glm_allvar_b_acc),
                                  Sensitivity = unname(glm_allvar_st),
                                  Specificity = unname(glm_allvar_sp), 
                                  stringsAsFactors = FALSE)
colnames(glm_allvar_results) <- metrics

results <- bind_rows(results, glm_allvar_results)
results %>% knitr::kable()

```


\pagebreak

## Linear Discriminant Analysis Model - All Variables

As we can see, our previous model gave us a significant improvement from our baseline results. Now, we will test an $LDA$ model as it is a relatively simplistic approach that will help us to sort out the amount of predictors we are using. Then we will store our results. Note that running this code can take several minutes depending on your computer characteristics.

```{r lda all vars}

fit_lda_allvar <- train(Class ~ ., method = "lda", data = train_set)

predict_lda_allvar <- predict(fit_lda_allvar, test_set)

cm_lda_allvar <- confusionMatrix(data = predict_lda_allvar, reference = test_set$Class,
                                positive = "1")

lda_allvar_acc <- cm_lda_allvar$overall["Accuracy"]
lda_allvar_b_acc <- cm_lda_allvar$byClass["Balanced Accuracy"]
lda_allvar_st <- cm_lda_allvar$byClass["Sensitivity"]
lda_allvar_sp <- cm_lda_allvar$byClass["Specificity"]

lda_allvar_results <-  data.frame(Model = "LDA - All Variables",
                                  Accuracy = unname(lda_allvar_acc),
                                  "Balanced Accuracy" = unname(lda_allvar_b_acc),
                                  Sensitivity = unname(lda_allvar_st),
                                  Specificity = unname(lda_allvar_sp), 
                                  stringsAsFactors = FALSE)
colnames(lda_allvar_results) <- metrics

results <- bind_rows(results, lda_allvar_results)
results %>% knitr::kable()

```

As we can see, all our metrics except Specificity (which was sligthly affected) got a significant improvement. This is because this model asumes the correlation structure is the same accross all predictors and it is not significantly affected by a large amount of predictors, and as we have some variables that indeed follow a fairly distinct distribution between transaction types, it helped out the model and leaves room for improvement if we pick this model as our final model, as we can drop some variables that have little to no predictive power, therefore reducing the "noise" within the model.

\pagebreak

## Quadratic Discriminant Analysis Model - All Variables

Taking into account that we had a significant improvement using an $LDA$ model, we will now try with a similar approach, more specifically, we will use an $QDA$ model, which is a version of Naive Bayes and assumes we have a multivariate normal distribution, which is true within some variables. Because of that fact, by using this model we should see an improvement over the results of the $LDA$ approach, as it is more restrictive and it does not allow flexibility within the distribution of the data. Again, after training and testing our model, we will store the results into our chart.

```{r qda all vars}

fit_qda_allvar <- train(Class ~ ., method = "qda", data = train_set)

predict_qda_allvar <- predict(fit_qda_allvar, test_set)

cm_qda_allvar <- confusionMatrix(data = predict_qda_allvar, reference = test_set$Class,
                                 positive = "1")

qda_allvar_acc <- cm_qda_allvar$overall["Accuracy"]
qda_allvar_b_acc <- cm_qda_allvar$byClass["Balanced Accuracy"]
qda_allvar_st <- cm_qda_allvar$byClass["Sensitivity"]
qda_allvar_sp <- cm_qda_allvar$byClass["Specificity"]

qda_allvar_results <-  data.frame(Model = "QDA - All Variables",
                                  Accuracy = unname(qda_allvar_acc),
                                  "Balanced Accuracy" = unname(qda_allvar_b_acc),
                                  Sensitivity = unname(qda_allvar_st),
                                  Specificity = unname(qda_allvar_sp), 
                                  stringsAsFactors = FALSE)
colnames(qda_allvar_results) <- metrics

results <- bind_rows(results, qda_allvar_results)
results %>% knitr::kable()

```

We in fact got another pretty good improvement in our results. This time, the Specificity was quite affected, but we are still maintaining it above 0.95. This is probably because we are using all the available variables as predictors, throwing noise into our model. 

It is very likely that we see another nice improvement in our results because of the flexibility the $QDA$ gives us when compared to the $LDA$ approach, and still, we do have some room for improvement if we go with this model.

\pagebreak

## Choosing a Final Model - Naive Predictor Picking

When we analyse the results we got from the three models we trained previously, we can see that the $QDA$ approach is the best one to work upon, as it allows by nature more flexibility for the data and it still can be improved by simply remove the less relevant variables for the model. 

We will start by using the top 5 variables we got by our naive average differences by transaction type approach we built duting the Data Exploration section as predictors and see if we get some improvements on the results. Then, again, we will store them into our chart. Note that we will refer to this approach as NAD on the code.

```{r naive nad approach}

diff[1:5]

fit_qda_nad5 <- train(Class ~ V3 + V14 + V17 + V12 + V10,
                       method = "qda", data = train_set)

predict_qda_nad5 <- predict(fit_qda_nad5, test_set)

cm_qda_nad5 <- confusionMatrix(data = predict_qda_nad5, reference = test_set$Class,
                                 positive = "1")

qda_nad5_acc <- cm_qda_nad5$overall["Accuracy"]
qda_nad5_b_acc <- cm_qda_nad5$byClass["Balanced Accuracy"]
qda_nad5_st <- cm_qda_nad5$byClass["Sensitivity"]
qda_nad5_sp <- cm_qda_nad5$byClass["Specificity"]

qda_nad5_results <-  data.frame(Model = "QDA - NAD Top 5",
                                  Accuracy = unname(qda_nad5_acc),
                                  "Balanced Accuracy" = unname(qda_nad5_b_acc),
                                  Sensitivity = unname(qda_nad5_st),
                                  Specificity = unname(qda_nad5_sp), 
                                  stringsAsFactors = FALSE)
colnames(qda_nad5_results) <- metrics

results <- bind_rows(results, qda_nad5_results)
results %>% knitr::kable()

```

As we can see, we got a slight improvement on our overall accuracy, probably by the increase on the $Specificity$, but we did not achieved any significant progress relative to our previous model. We will try now using the `filterVarImp` function to select another set of predictors. We will apply that function over the $QDA$ model we applied to all variables, then, we will train now on the top 10 variables get from the function. 

\pagebreak

## Choosing a Final Model - Variable Importance Predictor Picking 

We will start by selecting the top 10 variables we get when applying the `filterVarImp` function, and then we will train again our model using these variables as predictors. Depending on the result we get, we can also try to use only the top 5 variables from the function output. We will start by picking our variables from the $QDA - All\ Variables$ model, and then we will train the new model to store the new results into our chart.

```{r new predictors varimp model}

top_10_pred <- filterVarImp(test_set[-31], predict_qda_allvar)

top_10_pred <- top_10_pred[order(top_10_pred$X0, decreasing = TRUE),]

rownames(top_10_pred[1:10,])

fit_qda_fvi10 <- train(Class ~ V1 + V3 + V2 + V17 + V7
                           + V4 + V16 + V8 + V18 + V27,
                      method = "qda", data = train_set)

predict_qda_fvi10 <- predict(fit_qda_fvi10, test_set)

cm_qda_fvi10 <- confusionMatrix(data = predict_qda_fvi10, reference = test_set$Class,
                               positive = "1")

qda_fvi10_acc <- cm_qda_fvi10$overall["Accuracy"]
qda_fvi10_b_acc <- cm_qda_fvi10$byClass["Balanced Accuracy"]
qda_fvi10_st <- cm_qda_fvi10$byClass["Sensitivity"]
qda_fvi10_sp <- cm_qda_fvi10$byClass["Specificity"]

qda_fvi10_results <-  data.frame(Model = "QDA - filterVarImp Top 10",
                                Accuracy = unname(qda_fvi10_acc),
                                "Balanced Accuracy" = unname(qda_fvi10_b_acc),
                                Sensitivity = unname(qda_fvi10_st),
                                Specificity = unname(qda_fvi10_sp), 
                                stringsAsFactors = FALSE)
colnames(qda_fvi10_results) <- metrics

results <- bind_rows(results, qda_fvi10_results)
results %>% knitr::kable()

```

We got about the same result as our previous model, therefore, we will discard this predictor picking approach for our model and we will call the $QDA - All\ Variables$ our winner, as we got the best results from it. 

\pagebreak

# Results

After developing 6 models in total and doing some testing using approaches like $kNN$ or Decision Trees (because the run time was significantly affected on those models and we got about the same results as the $LDA - All\ Variables$ model, we will not include them into the report nor the Credit Card Fraud Detection Script file), we have a winner: the $QDA - All\ Variables$ model. We will quickly view our final results sorted by Sensitivity:

```{r final results}

final_results <- results[order(results$Sensitivity, decreasing = TRUE),]
final_results %>% knitr::kable() 

```

We can also see the detailed metrics of our final model viewing the Confusion Matrix by just calling the `cm_qda_allvar` object we created earlier:

```{r show confusion matrix}

cm_qda_allvar

```

\pagebreak

# Conclusions

Throughout this project, we learned some very useful insigths when we deal with a very unbalanced data. This project had some majors challenges we had to face and sort, for example: a very large amount of variables, variables without meaningful names, very high prevalece of one of the classes we need to predict. a somewhat large amount of observations, among others. 

Because of this, we chosed to go with a quite simplistic approach by using models that can handle a large amount of predictors and observations without compromising the runtime of our code, which resulted on a reduction of around $85\%$ of the fraudulent transactions.

We can also visualize the results we would get if we apply our model to the entire dataset (we will not consider this reuslts in our chart as we trained our data on $70\%$ of the complete dataset) with the following code:

```{r all dataset prediction}

qda_all_dataset <- predict(fit_qda_allvar, cc_dataset)

cm_qda_all_dataset <- confusionMatrix(data = qda_all_dataset, reference = cc_dataset$Class,
                                      positive = "1")
cm_qda_all_dataset

```

As a general conclusion for this project, the banking industry definetely has a big room for improvement on their fraudulent transactions detection systems. By improving them, they would significantly reduce the anual losses due to fraudulent credit card transactions, which could climb to millions, and by doing so, the cardholders can feel more confidence by shopping with their cards without worrying about getting their cards cloned, increasing the monetary mass on the economy.