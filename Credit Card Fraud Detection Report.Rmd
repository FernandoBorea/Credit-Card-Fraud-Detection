---
title: "Credit Card Fraud Detection"
author: "Fernando JosÃ© Velasco Borea"
date: "May 12th 2019"
output: 
  pdf_document:
      toc: true
      number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\pagebreak

# Introduction and Overview

An article conducted by Loss Prevention Magazine in 2018 showed that by 2020 the total monetary losses due to credit card fraud in the U.S. alone could excede the $10,000,000,000 mark (you can find the article [here](https://losspreventionmedia.com/credit-card-fraud-statistics-and-insights/)). With a constant growth on cardholders across the years, the concern about this type of fraud has also increased. On 2017 we saw an increment of 1.3 million credit card fraud victims, implying an increase of 8.4% when compared to the 2016 period (as reported by Javelin Strategy & Research). Taking this into account, I decided to conduct a supervised machine learning project with the goal of predicting potential fraudulent credit card transactions.

For this project, we will be using the data set provided by Machine Learning Group - ULB through Kaggle (you can find it through this [link](https://www.kaggle.com/mlg-ulb/creditcardfraud)). The data set contains information about the time (relative to the frequency of the transactions when compared to the first one in the data set), amount, type of transaction (either fraudulent or non-fraudulent, represented by a 1 or a 0 respectively) and 28 numerical features resulting from a PCA Dimensionality Reduction to protect the users identity and sensitive information.

The project will be divided into 4 major sections, as follows:

1. Data Adquisition
2. Data Exploration and Wrangling
3. Modeling
4. Testing

Once we complete the sections mentioned above, we will create a _Conclusions_ section with the insights we gathered throughout the project.

## Side Notes

Although the data set used for this project is downloaded within the code, to improve the run time, it is recommended to clone the [GitHub repository](https://github.com/FernandoBorea/Credit-Card-Fraud-Detection) as it contains the `csv` file with the data set we used.

To enhance code readability when viewing the Rmd version of this report and/or when viewing the Credit Card Fraud Detection Script file to see only the coding part of the project, you can _fold_ the all the sections from RStudio to then just _unfold_ the section you are currently viewing, therefore, easing the interpretation of the code.

You can quickly do this from RStudio going to _Edit > Folding > Collapse All_ or simply with the shortcut _ALT + O_ on windows. If you want to exapnd all the sections again, you can use the shortcut _ALT + SHIFT + O_ on windows or from _Edit > Folding > Expand All_.

The code contained in this report can be found on the Credit Card Fraud Detection Script file. It follows the same structure and order as the report, therefore, making it easier to reproduce the results while maintaining code readability.

To render the Rmd version of this report you will need to have a LaTeX installation. If you don't have it, you can find more details on how to install it [here](https://bookdown.org/yihui/rmarkdown/installation.html#installation).

\pagebreak

# Data Adquisition

This section is going be mainly intended to download or read the data set (depending if you have the repository cloned into your local machine) that we will be using throughout the project. 

First, we will start by loading the required libraries for our project, and then proceed to read our data either from our working directory if we cloned the repository, or from Git LFS if we have not. Note that because of formatting purposes, we will not show the output messages from the code below on the report. 

Executing this code section might take some minutes depending on your internet connection.

```{r data adquisition libraries, message=FALSE, warning=FALSE, error=FALSE}

if(!require(tidyverse)) install.packages("tidyverse", 
                                         repos = "http://cran.us.r-project.org")

if(!require(RCurl)) install.packages("RCurl", 
                                     repos = "http://cran.us.r-project.org")

if(!require(knitr)) install.packages("knitr", 
                                     repos = "http://cran.us.r-project.org")

if(!require(caret)) install.packages("caret", 
                                     repos = "http://cran.us.r-project.org")

if(!require(randomForest)) install.packages("randomForest", 
                                     repos = "http://cran.us.r-project.org")

if(file.exists("creditcard.csv"))
{
  
  cc_dataset <- read_csv("creditcard.csv")
  
} else {
  
  URL_p1 <- "https://media.githubusercontent.com"
  URL_p2 <- "/media/FernandoBorea/Credit-Card-Fraud-Detection/master/creditcard.csv"
  datURL <- getURL(paste(URL_p1, URL_p2, sep = ""))
  
#We divided the entire URL in 2 string vectors and 
#then used paste to maintain the report formatting
  
  cc_dataset <- read_csv(datURL)
  
}

```

\pagebreak

## Preliminary Data Exploration

Once the Data Adquisition process is finished, we will start performing some preliminary data exploration to make sure the data was downloaded and/or read correctly and to familiarize ourselves with the data set. 

When calling the function `str()` to look for the data structure, it will result in quite a large and somewhat messy output within our report. We already know from the [Kaggle Site](https://www.kaggle.com/mlg-ulb/creditcardfraud) where we got our data from, that we have several columns in our data set, therefore we are not going to include the output of the code below.

```{r preliminary exploration 1, results='hide'}

str(cc_dataset)

```

As we did not show the output of the code above, we will use another approach to still show some information about the data set within this section, more specifically, we will just check the amount of rows and columns as well as the class of each column:

```{r preliminary exploration 2}

data.frame(Columns = ncol(cc_dataset), Rows = nrow(cc_dataset)) %>% 
  knitr::kable()

col_classes <- data.frame(Column = colnames(cc_dataset)[1:16],
                 Class = unname(apply(cc_dataset, 2, class))[1:16],
                 Column = c(colnames(cc_dataset)[17:ncol(cc_dataset)],""),
                 Class = c(unname(apply(cc_dataset, 2, class))[17:ncol(cc_dataset)],""))

colnames(col_classes) <- c("Column", "Class", "Column","Class")

col_classes %>% knitr::kable()

```

\pagebreak

# Data Exploration and Wrangling

Now, as we finished the Data Adquisition phase, we will dive into our data set to gather useful insights and perform some data wrangling if necessary for the Modeling phase. As we saw from our Preliminary Data Exploration section, we are dealing with a slightly large amount of variables.

If we evaluate the chart containing the classes for each column on the previous section, we can notice that all of them contain numeric values, but the last one (the Class column) contains a binary value, either 1 or 0 for fraudulent or non-fraudulent transactions respectively. For this reason, we need to convert it from numeric to factor. We can do this using the following code:

```{r data wrangling: class as factor}

cc_dataset <- cc_dataset %>% mutate(Class = as.factor(cc_dataset$Class))

class(cc_dataset$Class)

```

Next, we are going to explore the data with some plots. We will start by cheking out the distribution of the `Class` variable. We can do that with the follwing code:

```{r fraudulent transactions distribution, out.width = '85%', fig.align='center'}

cc_dataset %>%
  count(Class) %>%
  ggplot(aes(x = Class, y = n/100, fill = Class)) + 
  geom_col(col = "Black") + 
  scale_y_log10() + 
  labs(title = "Fraudulent Transactions Distribution", 
       x = "Transaction Type (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "Count in Hundreds")

```

We can inmediately see a huge difference on the data distribution. Because of this, we will now check how many non-fraudulent transactions and how many fraudulent transactions we have in our data.

```{r class count}

class_dist <- cc_dataset %>%
               group_by(Class) %>%
               summarize(Count = n())
class_dist %>% knitr::kable()

```

The previous result tells us that we are facing a data set with a massively unbalanced distribution on the `Class` column. Because of this, we are going to take a new approach in our data exploration. In order to avoid creating 30 plots to check the distribution of each variable relative to the Class column, we are going to group each variable by the type of the observation to then compute the average on each group, then, we will visualize the distribution of the ones that have the biggest difference on the averages. 

```{r avg differences}

#Because of formatting purposes, we will not print out this object

vars_grouped_avgs <- cc_dataset %>%
                     group_by(Class) %>%
                     summarize_all(list(mean))

#We will drop the class column as it is a factor and we cannot perform operations with it

vars_diff <- vars_grouped_avgs[,-1]

#Then, we will calculate the difference bewtween values and sort them

diff <- abs(vars_diff[1,] - vars_diff[2,])
diff <- sort(diff, decreasing = TRUE)
diff

```

\pagebreak

Now, as we can see, the biggest differences were on the `Time` and `Amount` variables, but when we look at those entries on the `vars_grouped_avgs` data frame, we notice that we might have a very similar distribution as the difference between the averages on those cases is not that large when we take into account the magnitude of the values. 

```{r time and amount lookup}

vars_grouped_avgs[c("Time", "Amount")]

```

We can check if our hyphothesis is correct by plotting those variables and compare the actual distribution when grouped by the transaction type.

```{r hypthothesis check, out.width='75%', fig.align='center'}

#We will transform Amount variable to log10 values and tidy up the data 

time_amount_tidy <- cc_dataset[c("Time", "Amount", "Class")] %>%
  mutate(Amount = if_else(Amount != 0,log10(Amount), Amount)) %>%
  gather(-Class, key = "Variable", value = "Values")

#To then plot it

time_amount_tidy %>% 
  ggplot(aes(x = Class, y = Values, fill = Class)) +
  facet_wrap(~Variable, scales = "free") +
  geom_boxplot() + 
  labs(title = "Time and Amount Variables Distribution", 
       x = "Transaction Type (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "value")

```

As we can see, our hyphothesis was correct as we see a very similar distribution on both variables when we plot them grouped by the transaction type. This fact makes them have less relevance as predictors, so we will drop them from our vector.

```{r time and amount drop}

diff <- diff[-which(names(diff) %in% c("Time", "Amount"))]

```

Once we have updated the `diff` vector, we will make a plot comparing the value distribution of the new top 3 variables we got from our calculation, once again, dividing the data into fraudulent and non-fraudulent transactions:

```{r plot top avg vars, fig.align='center', out.width='85%'}

#We will tidy up the data first

tidy_data <- cc_dataset[c("V3", "V14", "V17", "Class")] %>%
  gather(-Class, key = "Variable", value = "Values")

#And then plot it

tidy_data %>% ggplot(aes(x = Class, y = Values, fill = Class)) + 
  facet_wrap(~Variable, scales = 'free') +
  geom_boxplot() + 
  labs(title = "V3, V14 and V17 Variables Distribution", 
       x = "Transaction Type (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "value")

```

As we can tell from the previous plots we generated, the Interquartile Ranges of the data distribution when we group by transaction type are well separated from each other, meaning that we can in fact have some predictive power from those variables, validating our initial approach of calculating the variable averages difference when grouped by transaction type.

\pagebreak

Now, another study we can perform with our data to corroborate that our approach is correct, is to make the same plots but this time with the last 3 variables from our `diff` vector and see if we in fact get a very similar distribution, meaning that those variables have less predictive power. We can perform this with the following code:

```{r plot tail avg vars, fig.align='center', out.width='75%'}

#We will again tidy up the data first

tidy_data <- cc_dataset[c("V25", "V23", "V22", "Class")] %>%
  gather(-Class, key = "Variable", value = "Values")

#And then plot it

tidy_data %>% ggplot(aes(x = Class, y = Values, fill = Class)) + 
  facet_wrap(~Variable, scales = 'free') +
  geom_boxplot() + 
  labs(title = "V25, V23 and V22 Variables Distribution", 
       x = "Transaction Type (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "value")

```

As we can see, our approach seems to hold up, as the data on the last 3 variables on our vector have a very similar Interquartile Range, therefore, providing low predictive power. Lastly on this section, we will perform some aditional data wrangling prior starting our modeling phase, more specifically, we will create a training set as well as a test set. We can do this with the following code:

```{r train and test set creation}

y <- cc_dataset$Class

#We will use 70% of the data for training and 30% for testing

train_index <-  createDataPartition(y, times = 1, p = 0.7, list = FALSE)

train_set <- cc_dataset %>% slice(train_index)
test_set <- cc_dataset %>% slice(-train_index)

```

# Modeling

## General Approach

As we noticed on the Data Exploration phase, we are dealing with a very unbalanced data distribution, so evaluating our model just based on accuracy is not viable because if we predict all transactions as non-fraudulent (which would cost billions in losses to the banking industry), we would get an accuracy of $\sim 99.83\%$, a $Sensitivity = 0$ as we would predict all transactions as negatives (therefore, we would never get a true positive), and a $Specificity = \sim99.83$ as 99.83% of our data would get a true negative prediction. We can corroborate this by checking what proportion of the data is clasified as non-fraudulent transactions:

```{r non fraudulent prop}

base_accuracy <- class_dist$Count[1] / nrow(cc_dataset)
base_accuracy

base_sensitivity <- 0
base_sensitivity

base_specificity <- sum(cc_dataset$Class == 0) / nrow(cc_dataset)
base_specificity

```

Taking this into account, we can also evaluate our model using the Sensitivity value (or True Positive Rate), as our final goal is to predict as much true fraudulent transactions as we can. We can set our baseline results as the ones we would get by predicting all transactions as non-fraudulent. 

We will focus on getting a $Sensitivity >= 0.95$ while maintaining also a $Specificity >= 0.95$ so we also avoid alerting about too many false positives. Our baseline would be as follows:

```{r baseline results}

results <- data.frame(Model = "All Non-Fraudulent", 
                      Accuracy = base_accuracy, 
                      Sensitivity = base_sensitivity, 
                      Specificity = base_specificity)
results %>% knitr::kable()

```

We will build initially 3 models based on all the variables, then we will choose the best among those and tune-up the variables we will be using as predictors as we clearly saw that some of them have little to no predictive power.

\pagebreak

We will use these 3 models to decide which one we will use for the final model:

1. Generalized Linear Model
2. Decision Tree Model
3. kNN Model

## Generalized Linear Model - All Variables

As explained on the General Approach section, we will now train a $GLM$ model using all the available variables to get a sense of the model performance we would get if we decide to use a Generalized Linear Model as a final model. Note that we will get this warning message: `fitted probabilities numerically 0 or 1 occurred`, this is due to the amount of predictors we are using, and as we are not going to consider in this case the Coefficients we get from our fitted model, we can disregard it.

```{r glm model allvar}

fit_glm_allvar <- glm(Class ~ ., data = train_set, family = "binomial")

predict_glm_allvar <- predict(fit_glm_allvar, test_set, type = "response")

yhat_glm_allvar <- if_else(predict_glm_allvar > 0.5, 1, 0) %>% factor()

cm_glm_allvar <- confusionMatrix(data = yhat_glm_allvar, reference = test_set$Class, 
                                 positive = "1")

glm_allvar_acc <- cm_glm_allvar$overall["Accuracy"]
glm_allvar_acc

glm_allvar_st <- cm_glm_allvar$byClass["Sensitivity"]
glm_allvar_st

glm_allvar_sp <- cm_glm_allvar$byClass["Specificity"]
glm_allvar_sp

```

Fuck this shit, will continue tomorrow.

\pagebreak

# Results

\pagebreak

# Conclusions