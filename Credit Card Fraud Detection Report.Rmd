---
title: "Credit Card Fraud Detection"
author: "Fernando JosÃ© Velasco Borea"
date: "May 12th 2019"
output: 
  pdf_document:
      toc: true
      number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\pagebreak

# Introduction and Overview

An article conducted by Loss Prevention Magazine in 2018 showed that by 2020 the total monetary losses due to credit card fraud in the U.S. alone could excede the $10,000,000,000 mark (you can find the article [here](https://losspreventionmedia.com/credit-card-fraud-statistics-and-insights/)). With a constant growth on cardholders across the years, the concern about this type of fraud has also increased. On 2017 we saw an increment of 1.3 million credit card fraud victims, implying an increase of 8.4% when compared to the 2016 period (as reported by Javelin Strategy & Research). Taking this into account, I decided to conduct a supervised machine learning project with the goal of predicting potential fraudulent credit card transactions.

For this project, we will be using the data set provided by Machine Learning Group - ULB through Kaggle (you can find it through this [link](https://www.kaggle.com/mlg-ulb/creditcardfraud)). The data set contains information about the time (relative to the frequency of the transactions when compared to the first one in the data set), amount, type of transaction (either fraudulent or non-fraudulent, represented by a 1 or a 0 respectively) and 28 numerical features resulting from a PCA Dimensionality Reduction to protect the users identity and sensitive information.

The project will be divided into 4 major sections, as follows:

1. Data Adquisition
2. Data Exploration and Wrangling
3. Modeling
4. Testing

Once we complete the sections mentioned above, we will create a _Conclusions_ section with the insights we gathered throughout the project.

## Side Notes

Although the data set used for this project is downloaded within the code, to improve the run time, it is recommended to clone the [GitHub repository](https://github.com/FernandoBorea/Credit-Card-Fraud-Detection) as it contains the `csv` file with the data set we used.

To enhance code readability when viewing the Rmd version of this report and/or when viewing the Credit Card Fraud Detection Script file to see only the coding part of the project, you can _fold_ the all the sections from RStudio to then just _unfold_ the section you are currently viewing, therefore, easing the interpretation of the code.

You can quickly do this from RStudio going to _Edit > Folding > Collapse All_ or simply with the shortcut _ALT + O_ on windows. If you want to exapnd all the sections again, you can use the shortcut _ALT + SHIFT + O_ on windows or from _Edit > Folding > Expand All_.

The code contained in this report can be found on the Credit Card Fraud Detection Script file. It follows the same structure and order as the report, therefore, making it easier to reproduce the results while maintaining code readability.

To render the Rmd version of this report you will need to have a LaTeX installation. If you don't have it, you can find more details on how to install it [here](https://bookdown.org/yihui/rmarkdown/installation.html#installation).

\pagebreak

# Data Adquisition

This section is going be mainly intended to download or read the data set (depending if you have the repository cloned into your local machine) that we will be using throughout the project. 

First, we will start by loading the required libraries for our project, and then proceed to read our data either from our working directory if we cloned the repository, or from Git LFS if we have not. Note that because of formatting purposes, we will not show the output messages from the code below on the report. 

Executing this code section might take some minutes depending on your internet connection.

```{r data adquisition libraries, message=FALSE, warning=FALSE, error=FALSE}

if(!require(tidyverse)) install.packages("tidyverse", 
                                         repos = "http://cran.us.r-project.org")

if(!require(RCurl)) install.packages("RCurl", 
                                     repos = "http://cran.us.r-project.org")

if(!require(knitr)) install.packages("knitr", 
                                     repos = "http://cran.us.r-project.org")

if(!require(caret)) install.packages("caret", 
                                     repos = "http://cran.us.r-project.org")

if(!require(randomForest)) install.packages("randomForest", 
                                     repos = "http://cran.us.r-project.org")

if(file.exists("creditcard.csv"))
{
  
  cc_dataset <- read_csv("creditcard.csv")
  
} else {
  
  URL_p1 <- "https://media.githubusercontent.com"
  URL_p2 <- "/media/FernandoBorea/Credit-Card-Fraud-Detection/master/creditcard.csv"
  datURL <- getURL(paste(URL_p1, URL_p2, sep = ""))
  
#We divided the entire URL in 2 string vectors and 
#then used paste to maintain the report formatting
  
  cc_dataset <- read_csv(datURL)
  
}

```

\pagebreak

## Preliminary Data Exploration

Once the Data Adquisition process is finished, we will start performing some preliminary data exploration to make sure the data was downloaded and/or read correctly and to familiarize ourselves with the data set. 

When calling the function `str()` to look for the data structure, it will result in quite a large and somewhat messy output within our report. We already know from the [Kaggle Site](https://www.kaggle.com/mlg-ulb/creditcardfraud) where we got our data from, that we have several columns in our data set, therefore we are not going to include the output of the code below.

```{r preliminary exploration 1, results='hide'}

str(cc_dataset)

```

As we did not show the output of the code above, we will use another approach to still show some information about the data set within this section, more specifically, we will just check the amount of rows and columns as well as the class of each column:

```{r preliminary exploration 2}

data.frame(Columns = ncol(cc_dataset), Rows = nrow(cc_dataset)) %>% 
  knitr::kable()

col_classes <- data.frame(Column = colnames(cc_dataset)[1:16],
                 Class = unname(apply(cc_dataset, 2, class))[1:16],
                 Column = c(colnames(cc_dataset)[17:ncol(cc_dataset)],""),
                 Class = c(unname(apply(cc_dataset, 2, class))[17:ncol(cc_dataset)],""))

colnames(col_classes) <- c("Column", "Class", "Column","Class")

col_classes %>% knitr::kable()

```

\pagebreak

# Data Exploration and Wrangling

Now, as we finished the Data Adquisition phase, we will dive into our data set to gather useful insights and perform some data wrangling if necessary for the Modeling phase. As we saw from our Preliminary Data Exploration section, we are dealing with a slightly large amount of variables.

If we evaluate the chart containing the classes for each column on the previous section, we can notice that all of them contain numeric values, but the last one (the Class column) contains a binary value, either 1 or 0 for fraudulent or non-fraudulent transactions respectively. For this reason, we need to convert it form numeric to factor. We can do this using the following code:

```{r data wrangling: class as factor}

cc_dataset <- cc_dataset %>% mutate(Class = as.factor(cc_dataset$Class))

class(cc_dataset$Class)

```

Next, we are going to explore the data with some plots. We will start by cheking out the distribution of the `Class` variable. We can do that with the follwing code:

```{r fraudulent transactions distribution, out.width = '85%', fig.align='center'}

cc_dataset %>%
  count(Class) %>%
  ggplot(aes(x = Class, y = n/100, fill = Class)) + 
  geom_col(col = "Black") + 
  scale_y_log10() + 
  labs(title = "Fraudulent Transactions Distribution", 
       x = "Class (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "Count in Hundreds")

```

We can inmediately see a huge difference on the data distribution. Because of this, we will now check how many non-fraudulent transactions and how many fraudulent transactions we have in our data.

```{r class count}

class_dist <- cc_dataset %>%
               group_by(Class) %>%
               summarize(Count = n())
class_dist %>% knitr::kable()

```

The previous result tells us that we are facing a data set with a massively unbalanced distribution on the `Class` column. Because of this, we are going to take a new approach in our data exploration. In order to avoid creating 30 plots to check the distribution of each variable relative to the Class column, we are going to group each variable by the class of the observation to then compute the average on each group, then, we will visualize the distribution of the ones that have the biggest difference on the averages. 

```{r avg differences}

#Because of formatting purposes, we will not print out this object

vars_grouped_avgs <- cc_dataset %>%
                     group_by(Class) %>%
                     summarize_all(list(mean))

#We will drop the class column as it is a factor and we cannot perform operations with it

vars_diff <- vars_grouped_avgs[,-1]

#Then, we will calculate the difference bewtween values and sort them

diff <- abs(vars_diff[1,] - vars_diff[2,])
diff <- sort(diff, decreasing = TRUE)
diff

```

\pagebreak

Now, as we can see, the biggest differences were on the `Time` and `Amount` variables, but when we look at those entries on the `vars_grouped_avgs` data frame, we notice that we might have a very similar distribution as the difference between the averages on those cases is not that large when we take into account the magnitude of the values. 

```{r time and amount lookup}

vars_grouped_avgs[c("Time", "Amount")]

```

We can check if our hyphothesis is correct by plotting those variables and compare the actual distribution when grouped by the transaction type.

```{r hypthothesis check, out.width='75%', fig.align='center'}

#We will transform Amount variable to log10 values and tidy up the data 

time_amount_tidy <- cc_dataset[c("Time", "Amount", "Class")] %>%
  mutate(Amount = if_else(Amount != 0,log10(Amount), Amount)) %>%
  gather(-Class, key = "Variable", value = "Values")

#To then plot it

time_amount_tidy %>% 
  ggplot(aes(x = Class, y = Values, fill = Class)) +
  facet_wrap(~Variable, scales = "free") +
  geom_boxplot() + 
  labs(title = "Time and Amount Variables Distribution", 
       x = "Class (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "value")

```

As we can see, our hyphothesis was correct as we see a very similar distribution on both variables when we plot them grouped by the transaction type. This fact makes them have less relevance as predictors, so we will drop them from our vector.

```{r time and amount drop}

diff <- diff[-which(names(diff) %in% c("Time", "Amount"))]

```

Once we have updated the `diff` vector, we will make a plot comparing the value distribution of the new top 3 variables we got from our calculation, once again, dividing the data into fraudulent and non-fraudulent transactions:

```{r plot avg vars, fig.align='center'}

#We will tidy up the data first

tidy_data <- cc_dataset[c("V3", "V14", "V17", "Class")] %>%
  gather(-Class, key = "Variable", value = "Values")

#And then plot it

tidy_data %>% ggplot(aes(x = Class, y = Values, fill = Class)) + 
  facet_wrap(~Variable, scales = 'free') +
  geom_boxplot() + 
  labs(title = "V3, V14 and V17 Variables Distribution", 
       x = "Class (0 = Non-Fraudulent, 1 = Fraudulent)",
       y = "value")

```

\pagebreak

# Modeling


\pagebreak

# Testing

\pagebreak

# Conclusions